# 集成学习的种类

## Boosting

* 训练基分类器时采用串行式，每一层训练时对前一层分错的样本给予更高的权重
* 测试时，根据各层的结果加权得到最终的结果

## Bagging

* 训练过程中，各基分类器之间没有强依赖关系，可以进行并行训练
* 为了使基分类器之间互相独立，训练集被分为若干子集（样本较少时，子集之间会有重叠）



# 集成学习的步骤和例子

* 集成学习一般可以分为三步：

  1. 找到误差相互独立的基分类器
  2. 训练基分类器
  3. 合并基分类器的结果

  

  + 合并分类器的办法：

    **voting**： 投票的方式，获得最多选票的结果作为最终结果

    **stacking**：前一个基分类器的结果输出到下一个分类器，所有基分类器的的结果相加作为最后输出

    ​				（或者更复杂的算法融合，比如将基分类器的输出作为特征，使用逻辑回归作为融合模型预测最后结果）

## Adaboost

训练与合并的步骤：

1. 选取ID3决策树作为基分类器，树形模型结构简单且较易产生随机性故尔较为常用

2. 训练基分类器：

   训练集为 $\{ x_i,y_i \},i=1...N$, 其中 $y_i \in \{-1, 1\}$, 有 $T$ 个基分类器

   * 初始化采样分布 $D_1(i)=1/N$

   * 令$t = 1, 2, ..., T$ 循环：

     * 从训练集中， 按照$D_t$分布，采样出子集 $S_t = \{x_i,y_i\}, i=1,...,N_t$

     * 用 $S_t$训练基分类器 $h_t$

     * 计算 $h_t$的错误率：
       $$
       \epsilon_t=\frac{\sum_{i=1}^{N_t}I[h_t(x_i)\neq y_i]D_t(x_i)}{N_t}
       $$
       $I$ 为判别函数

     * 计算基分类器 $h_t$的权重:

     $$
     a_t = \log \frac{(1-\epsilon_t)}{\epsilon_t}
     $$

     * 设置下一次采样

     $$
     D_{t+1} = \begin{cases}D_t(i)或\frac{D_t(i)(1-\epsilon_t)}{\epsilon_t},\ \ \ \ h_t(x_i)\neq y_i\\\frac{D_t(i)\epsilon_t}{(1-\epsilon_t)},\quad \quad \quad \quad \ \ \ \ \ \ h_t(x_i)=y_i \end{cases}
     $$

     并将其归一化为一个概率分布函数

3. 合并基分类器，给定一个未知样本 $z$ , 分类结果为加权投票的结果 $$Sign(\sum_{t=1}^Th_t(z)a_t)$$



# 基分类器的选择

决策树常用的原因：

1. 训练过程中可以较为方便的将样本权重整合，不需要使用过采样调整样本权重
2. 可以通过调整树的层数来调整决策树的表达能力和泛化能力
3. 数据样本的扰动对决策树的影响较大，因此不同训练子集形成的决策树之间随机性较大

除了决策树外，神经网络模型也适合做基分类器



# 偏差和方差



* **偏差**（bias）

  + 偏差指算法的期望预测与真实值之间的偏差程度，反映了模型本身的拟合能力
  + 通常是由于我们对学习算法做了错误的假设导致
  + 偏差带来的误差通常在训练误差上就能体现出来

  

* **方差**（variance）

  + 方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响。
  + 通常是由于模型复杂度相对训练样本数过高导致
  + 体现在测试误差相对于训练误差的增量上



# 从减小方差和偏差的角度解释boosting和bagging



* boosting减小偏差

  训练好一个基分类器后，将其错误或残差传递给下一个分类器

  这个过程就是在减小损失函数，使得模型不断逼近真实值，降低偏差

  但是不会降低方差，因为各个基分类器之间是强相关性的，缺乏独立性

  

* bagging减小方差

  $n$ 个随机变量             $\sigma^2$ 方差            $\rho$ 两两变量之间的相关性

  n个随机变量的均值为：
  $$
  \frac{\sum X_i}{n}
  $$
  方差为：
  $$
  \rho*\sigma^2 + (1-\rho)*\sigma^2/n
  $$
  在随机变量之间完全独立的情况下，方差变为 $\sigma^2/n$, 即变为了原来的 $1/n$

  

  

  

  



