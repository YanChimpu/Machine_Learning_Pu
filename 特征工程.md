# 特征归一化

## 做特征归一化的原因

* 消除特征之间的量纲影响，使得数据指标处于同一数量级

* 归一化的目的就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]）

  从而消除**奇异样本数据**导致的不良影响。

  + 奇异样本数据是指相对于其他输入样本特别大或特别小的样本矢量（即特征向量）

* 如果不进行归一化，那么由于特征向量中不同特征的取值相差较大，会导致目标函数变“扁”。

  这样在进行梯度下降的时候，梯度的方向就会偏离最小值的方向，走很多弯路，即训练时间过长。

![](/Users/pu/Documents/note/Machine_Learning_Pu/figures/特征工程/20171027225501116 .png)

* 如果进行归一化以后，目标函数会呈现比较“圆”，这样训练速度大大加快，少走很多弯路。

  ![](/Users/pu/Documents/note/Machine_Learning_Pu/figures/特征工程/20171027231904618.png)

  

  ## 归一化的好处

  1. 归一化后加快了梯度下降求最优解的速度

  2. 归一化有可能提高精度（如KNN）







## 归一化的方法

* 线性归一化：
  $$
  X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}
  $$


* 零均值归一化：
  $$
  z = \frac{x-\mu}{\sigma}
  $$





## 归一化适用范围

* 通过梯度下降法求解的模型通常是需要归一化的。如：

  线性回归、逻辑回归、SVM、神经网络等





# 文本表示模型

## 词袋模型（Bag of Words）和 N-gram 模型

* 词袋模型：

  + 将每篇文章视作一袋子词，忽略词出现的顺序。

  + 每篇文章表示成一个长向量，向量中的每一个维代表一个单词，

    该维的权重代表了这个词在原文中的重要程度。常用 **TF-IDF** 来计算权重
    $$
    TF-IDF(t,d)=TF(t,d)\times IDF(t)
    $$

    + $TF(t,d)$ ：单词 $t$ 在文档 $d$ 中出现的频率

    + $IDF(t)$ ：逆文档频率，用于衡量单词$t$对表达语义的重要性
      $$
      IDF(t) = log\frac{文章总数}{包含单词的文章总数+1}
      $$

* N-gram：

  将连续的 $n$ （$n \leq N$）个词组成的词组（**N-gram**) 作为单独特征放到向量表示中。



## 词嵌入与深度学习模型

* 词嵌入模型：
  + 将每个词映射到低维空间（通常 $K\in(50,300)$ ）上的一个稠密向量。
  + 词向量$K$维空间的每一维可以看作是一个隐藏的主题

## Word2Vec

[详细见[Word2Vec中的数学原理详解]](https://www.cnblogs.com/peghoty/p/3857839.html)

