# Gradient Boosting

输入：

训练数据集  $T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}, \quad x_i\in\varkappa\in R^n,\quad y_i\in\R$

损失函数 $L(y,f(x))$

输出： 回归树 $\hat{f}(x)$



1. 初始化
   $$
   f_0(x) = arg\ \mathop{min}\limits_{c}\sum_{i=1}^NL(y_i,c)
   $$

2. 对于 $m = 1,2,...,M$

   * 对于 $n = 1,2,...,N$, 计算：
     $$
     r_{mi}=-\left[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}\right]_{f(x)=f_{m-1}(x)}
     $$

   * 对 $r_{mi}$ 拟合一个回归树，得到第$m$颗树的叶节点区域 $R_{mj}$,   $j=1,2,..,J$

   * 对于$j=1,2,..,J$ 计算
     $$
     c_{mj} = arg\ \mathop{min}\limits_{c}\sum_{x_i\in R_{mj}}L(y_i, f_{m-1}(x_i)+c)
     $$

   * 更新
     $$
     f_{m}(x) = f_{m-1} + \sum_{j = 1}^Jc_{mi}I(x\in R_{mj})
     $$

3. 最后得到回归树
   $$
   \hat{f}(x) = f_{M}(x) = \sum_{m=1}^M\sum_{j=1}^Jc_{mj}I(x\in R_{mj})
   $$
   

# GBDT和XGBoost

[GBDT详解](https://zhuanlan.zhihu.com/p/30339807)

